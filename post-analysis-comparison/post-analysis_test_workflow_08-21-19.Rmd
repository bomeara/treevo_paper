---
title: "Post-Analysis Evaluation of Simulations for TreEvo"
author: "David Bapst"
date: "August 13, 2019"
output: pdf_document
---

Our 







```{r collapse=TRUE}
library(ape)
library(TreEvo)

# get package versions
if(packageVersion("TreEvo") < "0.21.0"){
	stop("Update TreEvo first!")
	}
	
message(paste0(
	"TreEvo Version Used: ", 
	packageVersion("TreEvo")
	))
message(paste0(
	"ape Version Used: ", 
	packageVersion("ape")
	))

```


First thing first, what directory do we want to get data files from?

```{r}
dir <- "~//treevo_paper//analyses_cluster_fast//"
```

Remove 


```{r}

# see all files in working directory
files <- list.files(dir)
# identify all .rda files
filesRDA <- files[grep(pattern=".rda",files)]

# identify all data files
	# only need to load one of these to get workspace
	# this should be arbitrary
filesData <- filesRDA[grep(pattern="Data_",filesRDA)]
# load first one
load(paste0(dir,filesData[1]))

# get all results files
filesResults <- filesRDA[grep(pattern="Results_",filesRDA)]

# get names of the analyses
analysisNames_Results <- sapply(filesResults,
                                function(x){
                                  x <- gsub(x, pattern="Results_", replacement="")
                                  x <- gsub(x, pattern=".rda", replacement="")
                                  # remove date and last '_'
                                  substr(x, start = 1, stop = nchar(x)-11)
                                  }
                                )

# for now, just crash if there's duplicates
if(length(analysisNames_Results) > length(unique(analysisNames_Results))){
	stop("There's duplicate results files")
	}

# need to load results sequentially
	# place as a sub-object in the pre-existing list: analysisOutput

for(i in 1:length(analysisNames_Results)){
  fileLoc <- paste0(dir, filesResults[i])
  #print(fileLoc)
	load(fileLoc)
	analysisOutput[[ analysisNames_Results[i] ]] <- results
}

# remove all empty runs that are not a list
analysisOutput <- analysisOutput[sapply(analysisOutput, is.list)]

```



# Effective Sample Size

 This function calculates Effective Sample Size (ESS) on results.  

 Performs the best when results are from multiple runs.

```{r}
ESS <- as.list(names(analysisOutput))
names(ESS) <- names(analysisOutput)

for(i in 1:length(analysisOutput)){
  print(names(analysisOutput))
  ESS[[i]] <- suppressMessages(pairwiseESS(analysisOutput[[i]]))
  print(ESS[[i]])
  #
}

```


# testMultivarOutlierHDR

This tests if an 'outlier' (some sample) is within a multivariate cloud of particles at some alpha

Very useful for testing if the generating parameters are within the particles for a simulation for dependant analyses - not so useful for indep analyses though!

```{r eval = FALSE}
particleMatrix <- NA
generatingParams <- NA

testMultivarOutlierHDR(
  dataMatrix = particleMatrix, 
  outlier = generatingParams, 
  alpha = 0.8, 
  pca = TRUE
  )
```

# plotUnivariatePosteriorVsPrior 

This function plots priors versus their posteriors - this will be useful for runs with bad prior on BM.

```{r}

for(i in 1:length(analysisOutput)){

 whichNonFixedPriors <- sapply(
    analysisOutput[[i]][[1]]$priorList,
      function(x) x$fun != "fixed"
      )
    
  npar <- length(whichNonFixedPriors)
      
  
  for(j in 1:npar){
  
    # first need to get prior and posterior kernals
    
    whichPrior <- whichNonFixedPriors[j]
  
    priorKernal<-getUnivariatePriorCurve(
      priorFn=analysisOutput[[i]][[1]]$priorList[[whichPrior]]$fun, 
      priorVariables=analysisOutput[[i]][[1]]$priorList[[whichPrior]]$params,
    	nPoints=100000, 
    	from=NULL, 
    	to=NULL, 
    	alpha=0.95
      )
    
    postKernal<-getUnivariatePosteriorCurve(
      acceptedValues=analysisOutput[[i]][[1]]$particleDataFrame[ , 6 + j],
    	from=NULL, 
      to=NULL, 
      alpha=0.95
      )
    
    plotUnivariatePosteriorVsPrior(
      posteriorCurve = postKernal, 
      priorCurve = priorKernal,
      label = "parameter")
    
  }
}

```
	

```{r}

```

# let's compare this (supposed) prior against the posterior in a plot

```{r}

```
	
# plotPosteriors
	# for each free parameter in the posterior, a plot is made of the distribution of values estimate in the last generation
	# can also be used to visually compare against true (generating) parameter values in a simulation.
	
plotPosteriors(particleDataFrame=resultsBM$particleDataFrame,
   priorsMat=resultsBM$PriorMatrix)

# highestPostDens 
	# get weighted mean, standard deviation, upper and lower highest posterior density (HPD) for each free parameter in posterior. 

highestPostDens(results$particleDataFrame, percent=0.95, returnData=FALSE)
	
# plotABC_3D
	# Plot posterior density distribution for each generation in 3d plot window 


#############################################################################################
# notes from conversation with Peter Smits (05-09-18)
#
# so I'm doing approximate bayesian computation and the question is, what do I want to show to the reader
# I want to show posterior parameter estimates from real data, 
	# and show that they are very different from parameter estimates made under other models,
	# or under the same model but with simulated data, for scenarios with a small number of models
# what i do is make the same series of posterior predictive checks
	# and demonstrate how your prefered model better recapitulates the data it was fit to
#
# 
############################################################
# ECDF
# ECDF - empirical cumulative distribution function = the ranked order accumulation curve
# http://stat.ethz.ch/R-manual/R-devel/library/stats/html/ecdf.html
# ecdf is a cool way of summarizing the entire dataset graphically
#
# how well does simulations under a fit model reproduce ecdf or the density of the original data? 
# if your model does a better job of doing that, then it is straight up a better model
# it also goes beyond parameter estimates and towards the model describing the data
#
# bayes wants to describe more than just the expected value. it is greedy and wants to describe the whole posterior
# the posterior predictive distribution describes all data sets that are consistent with the model, given the original input information
# if the PPD doesn't look like the empirical data, then the model is not describing your data
#####################################################################################
# from 06-21-18
# okay so the general sketch is particles from the posterior, simulate under this set of parameters N times, 
    # and compare the original ECDF for each parameter to the simulated
# my other idea:
# draw parameter estimates from some posterior particle, simulate under those parameters, 
    #  then test if 'true' generating parameters are actually within the 95% HDF of the simulated posterior
    # deals with how we don't really understand how adequate the models are for giving unbiased estimates of parameters

# checkAdequacy              # sixAnalysesTwoModels
    

#checkAdequacy <- function(){
#    }



#I mean, writing a function that just takes arguments: tree, params, etc. and returns results would be good

#a lot of this is (dataset) and six corresponding analyses

#fit model A, model B to real data, then simulate under model A, model B and fits both model A and B to both
#(where A is usually BM)


```

